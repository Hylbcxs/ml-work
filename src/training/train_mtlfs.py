import pdb
import argparse
import os
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
import random

import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.distributed as dist
import torch.optim
import torch.utils.data
import torch.utils.data.distributed
from torch.utils.data.distributed import DistributedSampler
import matplotlib.pyplot as plt
import numpy as np

from ..models.our.mtlfs_model import MTLFS_Model
from ..utils import init_logger, set_seed, plot_loss_curve
from ..data_loader.mtlfs_data_loader import mtlfs_dataloader

def parse_args():
    parser = argparse.ArgumentParser(description='MTLFS Model')
    
    # Model Parameters
    parser.add_argument('--input_feature', default=14, type=int, help='input feature size')
    parser.add_argument('--hidden_size', default=256, type=int, help='hidden size')
    parser.add_argument('--input_size', default=90, type=int, help='input sequence length')
    parser.add_argument('--output_size', default=90, type=int, help='short-term prediction length')
    parser.add_argument('--dropout', default=0.1, type=float, help='dropout rate')
    
    # Training Parameters
    parser.add_argument('--epochs', default=100, type=int, help='training epochs')
    parser.add_argument('--batch_size', default=32, type=int, help='batch size')
    parser.add_argument('--lr', default=5e-4, type=float, help='learning rate')
    parser.add_argument('--weight_decay', default=1e-4, type=float, help='weight decay')
    parser.add_argument('--loss_type', default='mse', type=str, help='loss type: mse, mae')
    
    # Validation Parameters
    parser.add_argument('--val_ratio', default=0.2, type=float, help='validation ratio (0.2 = 20%)')
    parser.add_argument('--early_stop_patience', default=20, type=int, help='early stopping patience')
    parser.add_argument('--val_check_interval', default=5, type=int, help='validation check interval (epochs)')
    
    # Data Parameters
    parser.add_argument('--data_path', required=True, type=str, help='data path')
    parser.add_argument('--mode', default='train', type=str, help='train or test')
    parser.add_argument('--prediction_mode', default='short', type=str, 
                       help='prediction mode: short, long')
    
    # Other Parameters
    parser.add_argument('--seed', default=42, type=int, help='random seed')
    parser.add_argument('--print_every', default=10, type=int, help='print frequency')
    parser.add_argument('--log_file', required=True, type=str, help='log file path')
    parser.add_argument('--save_path', required=True, type=str, help='model save path')
    parser.add_argument('--figure_path', required=True, type=str, help='figure save path')
    parser.add_argument('--num_workers', default=4, type=int, help='data loader workers')
    
    return parser.parse_args()


def train(args, model, experiment, prediction_type):
    output_size = args.output_size
    log_file = f"{args.log_file}/mtlfs/{prediction_type}/{args.loss_type}"
    os.makedirs(log_file, exist_ok=True)
    log = init_logger(f"{log_file}/train_{experiment}.log", mode='w')

    # åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
    local_rank = int(os.environ["LOCAL_RANK"])
    if experiment == 0:
        torch.distributed.init_process_group(backend="nccl")
    torch.cuda.set_device(local_rank)
    args.distributed = torch.distributed.is_initialized() if torch.distributed.is_available() else False
    args.nodes = torch.distributed.get_world_size()
    rank = dist.get_rank()

    # æ•°æ®åŠ è½½å™¨ - ä½¿ç”¨å•ä¸€é¢„æµ‹æ¨¡å¼
    train_loader, val_loader = mtlfs_dataloader(args, prediction_type, args.val_ratio)
    
    if rank == 0:
        log.info(f'MLTFS {prediction_type}æœŸé¢„æµ‹è®­ç»ƒå¼€å§‹')
        log.info(f'é¢„æµ‹é•¿åº¦: {output_size}å¤©')

    model = model.cuda()
    model = torch.nn.parallel.DistributedDataParallel(
        model, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=True
    )

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    if args.loss_type.lower() == 'mae':
        criterion = torch.nn.L1Loss().cuda()
    elif args.loss_type.lower() == 'mse':
        criterion = torch.nn.MSELoss().cuda()

    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=args.lr, 
        weight_decay=1e-4
    )
    
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 
        mode='min', 
        factor=0.5, 
        patience=10,
        verbose=True if rank == 0 else False
    )

    model.train()
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience_counter = 0
    best_epoch = 0

    if rank == 0:
        log.info("ğŸ å¼€å§‹è®­ç»ƒ...")
    
    i = 0
    for epoch in tqdm(range(args.epochs)):
        model.train()
        epoch_train_loss = 0.0
        train_batches = 0
        
        for inputs, labels in tqdm(train_loader):
            inputs = inputs.float().cuda()
            labels = labels.float().cuda()
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            predictions = model(inputs, prediction_type=prediction_type)
            loss = criterion(predictions, labels)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            epoch_train_loss += loss.item()
            train_batches += 1
            if i % 100 == 0:
            
                draw(args, predictions.permute(0, 2, 1).cpu().detach().numpy(), torch.cat([inputs, labels], dim=1).permute(0, 2, 1).cpu().detach().numpy(), experiment)
        
        avg_train_loss = epoch_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        if (epoch + 1) % args.val_check_interval == 0 or epoch == args.epochs - 1:
            avg_val_loss = validate_model(
                model.module if args.distributed else model, 
                val_loader, 
                criterion, 
                prediction_type
            )
            val_losses.append(avg_val_loss)
            scheduler.step(avg_val_loss)
            # æ—©åœæ£€æŸ¥
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                best_epoch = epoch
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if rank == 0:
                    save_path = f"{args.save_path}/mtlfs/{prediction_type}/{args.loss_type}"
                    os.makedirs(save_path, exist_ok=True)
                    torch.save(
                        model.module.state_dict() if args.distributed else model.state_dict(), 
                        f"{save_path}/mtlfs_model_{experiment}.pth"
                    )
                    log.info(f"ğŸ’¾ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (Epoch {epoch+1}, Val Loss: {avg_val_loss:.6f})")
            else:
                patience_counter += 1
            
            # æ—¥å¿—è®°å½•
            if rank == 0:
                current_lr = optimizer.param_groups[0]['lr']
                log.info(f"Epoch {epoch+1}/{args.epochs}")
                log.info(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}")
                log.info(f"  éªŒè¯æŸå¤±: {avg_val_loss:.6f}")
                log.info(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f} (Epoch {best_epoch+1})")
                log.info(f"  å­¦ä¹ ç‡: {current_lr:.8f}")
                log.info(f"  æ—©åœè®¡æ•°: {patience_counter}/{args.early_stop_patience}")
            # æ—©åœæ£€æŸ¥
            if patience_counter >= args.early_stop_patience:
                if rank == 0:
                    log.info(f"æ—©åœè§¦å‘ï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f} (Epoch {best_epoch+1})")
                break
        else:
            # ééªŒè¯epochåªè®°å½•è®­ç»ƒæŸå¤±
            if rank == 0 and (epoch + 1) % args.print_every == 0:
                current_lr = optimizer.param_groups[0]['lr']
                log.info(f"Epoch {epoch+1}/{args.epochs}")
                log.info(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}")
                log.info(f"  å­¦ä¹ ç‡: {current_lr:.8f}")
        # if rank == 0:
        #     scheduler.step()
            
        #     if (epoch + 1) % args.print_every == 0:
        #         log.info(f'Epoch {epoch + 1}/{args.epochs}, Loss: {avg_loss:.5f}')
                
        #         # ä¿å­˜æ¨¡å‹
        #         save_path = f"{args.save_path}/mltfs/{prediction_type}/{args.loss_type}"
        #         if not os.path.exists(save_path):
        #             os.makedirs(save_path)
        #         torch.save(model.module.state_dict(), 
        #                   f"{save_path}/mltfs_{prediction_type}_{experiment}_{epoch + 1}.pth")
    
    plot_tran_and_valid_loss(args, train_losses, val_losses, experiment, best_epoch, log_file)
        
    # plot_loss_curve(loss_list, f'{log_file}/save_loss_curve_{experiment}.png')

def validate_model(model, val_loader, criterion, prediction_type):
    """
    éªŒè¯æ¨¡å‹æ€§èƒ½
    
    Args:
        model: æ¨¡å‹
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        device: è®¾å¤‡
        prediction_type: é¢„æµ‹ç±»å‹
    
    Returns:
        avg_val_loss: å¹³å‡éªŒè¯æŸå¤±
    """
    model.eval()
    total_val_loss = 0.0
    num_batches = 0
    
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.float().cuda()
            labels = labels.float().cuda()
            
            # å‰å‘ä¼ æ’­
            predictions = model(inputs, prediction_type=prediction_type)
            loss = criterion(predictions, labels)
            
            total_val_loss += loss.item()
            num_batches += 1
    
    avg_val_loss = total_val_loss / num_batches
    return avg_val_loss

def plot_tran_and_valid_loss(args, train_losses, val_losses, experiment, best_epoch, log_file):
    plt.figure(figsize=(12, 5))
    
    # è®­ç»ƒæŸå¤±æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='training loss', color='blue')
    plt.title('training loss curve')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    # éªŒè¯æŸå¤±æ›²çº¿
    if val_losses:
        plt.subplot(1, 2, 2)
        val_epochs = [(i+1) * args.val_check_interval - 1 for i in range(len(val_losses))]
        if val_epochs[-1] != args.epochs - 1:  # å¦‚æœæœ€åä¸€ä¸ªepochæœ‰éªŒè¯
            val_epochs[-1] = len(train_losses) - 1
        plt.plot(val_epochs, val_losses, label='validation loss', color='red', marker='o')
        plt.axvline(x=best_epoch, color='green', linestyle='--', alpha=0.7, label=f'best model (Epoch {best_epoch+1})')
        plt.title('validation loss curve')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
    
    plt.tight_layout()
    # log_path = f"{args.log_file}/mtlfs_with_val/{prediction_type}/{args.loss_type}"
    # os.makedirs(log_path, exist_ok=True)
    plt.savefig(f"{log_file}/training_curves_{experiment}.png", dpi=300, bbox_inches='tight')
    plt.close()
    
def test(args, model, prediction_type, experiment):
    # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
    test_dataloader = mtlfs_dataloader(args, prediction_type)
    test_model_path = f"{args.save_path}/mltfs/{prediction_type}/{args.loss_type}/mtlfs_best_model_{experiment}.pth"
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    if test_model_path and os.path.exists(test_model_path):
        model.load_state_dict(torch.load(test_model_path))
    else:
        print("è­¦å‘Š: æœªæŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
    
    model = model.cuda()
    model.eval()

    if args.loss_type.lower() == 'mae':
        criterion = torch.nn.L1Loss().cuda()
    elif args.loss_type.lower() == 'mse':
        criterion = torch.nn.MSELoss().cuda()
    total_loss = 0
    all_predictions = []
    all_targets = []
    
    print("ğŸš€ å¼€å§‹é¢„æµ‹...")
    with torch.no_grad():
        for batch_idx, (inputs, labels) in enumerate(tqdm(test_dataloader)):
            inputs = inputs.float().cuda()
            labels = labels.float().cuda()
            
            # å‰å‘ä¼ æ’­
            predictions = model(inputs, prediction_type=prediction_type)
            loss = criterion(predictions, labels)

            all_predictions.append(predictions)
            all_targets.append(labels)
            total_loss += loss.item()

            draw(args, predictions.permute(0, 2, 1).cpu().detach().numpy(), torch.cat([inputs, labels], dim=1).permute(0, 2, 1).cpu().detach().numpy(), experiment)
    avg_loss = total_loss / len(test_dataloader)
    
    # åˆå¹¶æ‰€æœ‰é¢„æµ‹ç»“æœ
    all_predictions = torch.cat(all_predictions, dim=0)
    all_targets = torch.cat(all_targets, dim=0)
    
    return avg_loss
    
def draw(args, npList, label, experiment):
    # index = random.randint(0, args.batch_size-1)
    actual_batch_size = npList.shape[0]
    index = random.randint(0, actual_batch_size - 1)

    prediction_list = npList[index]
    label_list = label[index]

    fig = plt.figure(figsize=(15, 8))
    # legend = ['Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3', 'sub_metering_remainder', 'RR','NBJRR1','NBJRR5' ,'NBJRR10', 'NBJBROU', 'temperature_2m_mean']
    legend = ['Global_active_power']
    for i in range(1):
        # åˆ›å»ºä¸€ä¸ªæ–°çš„å­å›¾
        # if i >= 9:
        #     ax = fig.add_subplot(5, 3, i + 1)
        # else:
        #     ax = fig.add_subplot(5, 3, i + 1)
        ax = fig.add_subplot(1, 1, 1)

        # ç»˜åˆ¶è¿™ä¸ªç‰¹å¾éšæ—¶é—´å˜åŒ–çš„æ›²çº¿
        plt.sca(ax)
        x = np.arange(0, args.input_size+args.output_size)

        # import pdb; pdb.set_trace()
        plt.plot(x, label_list[i], label='Ground Truth')
        plt.plot(x[args.input_size:], prediction_list[i], label='Prediction')

        # æ·»åŠ æ ‡ç­¾å’Œå›¾ä¾‹
        plt.title(legend[i])
        plt.xlabel('Time Step')
        plt.ylabel('Feature Value')
        plt.legend()

    # ä¿å­˜å›¾åƒ
    plt.tight_layout()
    if args.output_size == 90:
        savefig_path = f"{args.figure_path}/mtlfs/{args.prediction_mode}/{args.loss_type}"
    elif args.output_size == 365:
        savefig_path = f"{args.figure_path}/mtlfs/{args.prediction_mode}/{args.loss_type}"
    if not os.path.exists(savefig_path):
        os.makedirs(savefig_path)
    plt.savefig(f"{savefig_path}/features_{experiment}.png")
    plt.close()
    

def main():
    args = parse_args()
    set_seed(args)
    all_losses = [] 

    for experiment in range(5):
        print(f"Starting experiment {experiment + 1}/5")
        args.mode = 'train'
        # åˆ›å»ºæ¨¡å‹
        model = MTLFS_Model(
            input_feature=args.input_feature,
            hidden_size=args.hidden_size,
            input_size=args.input_size,
            output_size=args.output_size,
            dropout=args.dropout
        )
        train(args, model, experiment, args.prediction_mode)

        args.mode = 'test'
        model = MTLFS_Model(
            input_feature=args.input_feature,
            hidden_size=args.hidden_size,
            input_size=args.input_size,
            output_size=args.output_size,
            dropout=args.dropout
        )
        avg_loss = test(args, model, args.prediction_mode, experiment)
        all_losses.append(avg_loss)
    print("\nAverage over 5 experiments:")
    print(f"Average Loss: {np.mean(all_losses):.5f} Â± {np.std(all_losses):.5f}")


if __name__ == '__main__':
    main() 