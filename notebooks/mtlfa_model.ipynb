{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b27c6e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c26ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTLFA_PowerDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, input_size, prediction_type='short', scaler_path=None):\n",
    "        self.input_size = input_size\n",
    "        self.prediction_type = prediction_type\n",
    "        self.output_size = 90 if prediction_type == 'short' else 365\n",
    "        self.scaler_path = scaler_path or f\"/opt/data/private/hyl/code/ml-work/data/scaler.pkl\"\n",
    "        \n",
    "        # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®\n",
    "        self._load_and_preprocess_data()\n",
    "        \n",
    "        # ç”Ÿæˆæ ·æœ¬\n",
    "        self.data_x = []\n",
    "        self.data_y = []\n",
    "        self._prepare_data()\n",
    "\n",
    "    def _load_and_preprocess_data(self):\n",
    "        \"\"\"åŠ è½½å’Œé¢„å¤„ç†æ•°æ®ï¼Œç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•ä½¿ç”¨ç›¸åŒçš„æ ‡å‡†åŒ–å‚æ•°\"\"\"\n",
    "            # è®­ç»ƒæ¨¡å¼ï¼šåŠ è½½è®­ç»ƒæ•°æ®å¹¶æ‹Ÿåˆscaler\n",
    "        train_df = pd.read_csv(\"/opt/data/private/hyl/code/ml-work/data/train_new.csv\")\n",
    "        train_df = self._clean_data(train_df)\n",
    "        \n",
    "        # æ‹Ÿåˆscalerå¹¶ä¿å­˜\n",
    "        self.scaler = RobustScaler()  # ä½¿ç”¨RobustScalerï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’\n",
    "        train_data = self.scaler.fit_transform(np.array(train_df))\n",
    "        \n",
    "        # ä¿å­˜scalerå‚æ•°\n",
    "        os.makedirs(os.path.dirname(self.scaler_path), exist_ok=True)\n",
    "        with open(self.scaler_path, 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        \n",
    "        self.data = train_data\n",
    "        print(f\"âœ… è®­ç»ƒæ¨¡å¼ï¼šæ‹Ÿåˆå¹¶ä¿å­˜scaleråˆ° {self.scaler_path}\")\n",
    "        \n",
    "        # æ•°æ®ç»Ÿè®¡\n",
    "        print(f\"ğŸ“Š æ•°æ®å½¢çŠ¶: {self.data.shape}\")\n",
    "        print(f\"ğŸ“Š æ•°æ®èŒƒå›´: [{self.data.min():.3f}, {self.data.max():.3f}]\")\n",
    "        print(f\"ğŸ“Š æ•°æ®å‡å€¼: {self.data.mean():.3f}\")\n",
    "        print(f\"ğŸ“Š æ•°æ®æ ‡å‡†å·®: {self.data.std():.3f}\")\n",
    "\n",
    "    def _clean_data(self, df):\n",
    "        \"\"\"æ•°æ®æ¸…ç†\"\"\"\n",
    "        df.replace('?', np.nan, inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        df = df.drop(columns=['DateTime'])\n",
    "        return df\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        \"\"\"å‡†å¤‡æ—¶é—´åºåˆ—æ ·æœ¬\"\"\"\n",
    "        window_size = self.input_size + self.output_size\n",
    "        \n",
    "        for index in range(len(self.data) - window_size + 1):\n",
    "            input_data = self.data[index:index + self.input_size]\n",
    "            label_data = self.data[index + self.input_size:index + window_size]\n",
    "            \n",
    "            self.data_x.append(input_data)\n",
    "            self.data_y.append(label_data)\n",
    "        \n",
    "        self.data_x = np.array(self.data_x)\n",
    "        self.data_y = np.array(self.data_y)\n",
    "        \n",
    "        print(f\"ğŸ“Š {self.prediction_type}æœŸé¢„æµ‹æ•°æ®å‡†å¤‡å®Œæˆ\")\n",
    "        print(f\"ğŸ“Š ç”Ÿæˆæ ·æœ¬æ•°: {len(self.data_x)}\")\n",
    "        print(f\"ğŸ“Š è¾“å…¥å½¢çŠ¶: {self.data_x.shape}\")  # åº”è¯¥æ˜¯ [N, 90, 14]\n",
    "        print(f\"ğŸ“Š æ ‡ç­¾å½¢çŠ¶: {self.data_y.shape}\")  # åº”è¯¥æ˜¯ [N, 90, 14]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_data = torch.tensor(self.data_x[idx], dtype=torch.float32)\n",
    "        label_data = torch.tensor(self.data_y[idx], dtype=torch.float32)\n",
    "        \n",
    "        return input_data, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7223982d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è®­ç»ƒæ¨¡å¼ï¼šæ‹Ÿåˆå¹¶ä¿å­˜scaleråˆ° /opt/data/private/hyl/code/ml-work/data/scaler.pkl\n",
      "ğŸ“Š æ•°æ®å½¢çŠ¶: (745, 14)\n",
      "ğŸ“Š æ•°æ®èŒƒå›´: [-5.668, 15.450]\n",
      "ğŸ“Š æ•°æ®å‡å€¼: 0.125\n",
      "ğŸ“Š æ•°æ®æ ‡å‡†å·®: 1.013\n",
      "ğŸ“Š shortæœŸé¢„æµ‹æ•°æ®å‡†å¤‡å®Œæˆ\n",
      "ğŸ“Š ç”Ÿæˆæ ·æœ¬æ•°: 566\n",
      "ğŸ“Š è¾“å…¥å½¢çŠ¶: (566, 90, 14)\n",
      "ğŸ“Š æ ‡ç­¾å½¢çŠ¶: (566, 90, 14)\n",
      "torch.Size([32, 90, 14]) torch.Size([32, 90, 14])\n"
     ]
    }
   ],
   "source": [
    "input_size, output_size = 90, 90\n",
    "batch_size = 32\n",
    "raw_dataset = MTLFA_PowerDataset(input_size)\n",
    "dataloader = DataLoader(raw_dataset, batch_size=batch_size, drop_last=True)\n",
    "for input, label in dataloader:\n",
    "    print(input.shape, label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3edc04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleCNN_TimeSeries(nn.Module):\n",
    "    def __init__(self, input_feature, hidden_size, kernel_sizes=[3, 7, 15], dropout=0.3):\n",
    "        super(MultiScaleCNN_TimeSeries, self).__init__()\n",
    "        \n",
    "        # ä½¿ç”¨å¤šä¸ªå·ç§¯æ ¸æ¥æ„å»ºå¤šå°ºåº¦å·ç§¯æ¨¡å—\n",
    "        # æ³¨æ„ï¼šæˆ‘ä»¬å°†å·ç§¯æ²¿ç€æ—¶é—´æ­¥(seq_len)è¿›è¡Œåº”ç”¨\n",
    "        self.conv1 = nn.Conv1d(input_feature, hidden_size, kernel_size=kernel_sizes[0], padding=kernel_sizes[0] // 2)\n",
    "        self.conv2 = nn.Conv1d(input_feature, hidden_size, kernel_size=kernel_sizes[1], padding=kernel_sizes[1] // 2)\n",
    "        self.conv3 = nn.Conv1d(input_feature, hidden_size, kernel_size=kernel_sizes[2], padding=kernel_sizes[2] // 2)\n",
    "\n",
    "        # Batch normalization å’Œ Dropout å±‚\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.residual_proj = nn.Linear(input_feature, hidden_size * 3)\n",
    "        # å…¨è¿æ¥å±‚ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰\n",
    "        # self.fc = nn.Linear(hidden_size * 3 * 90, 128)  # å‡è®¾åºåˆ—é•¿åº¦æ˜¯90ï¼Œåˆå¹¶ä¸‰ä¸ªå°ºåº¦çš„ç‰¹å¾å\n",
    "        # self.output_fc = nn.Linear(128, 1)  # è¾“å‡ºå±‚\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual_proj(x)\n",
    "        # x.shape = [batch_size, input_size, input_feature] --> [batch_size, input_feature, input_size] for Conv1D\n",
    "        x = x.transpose(1, 2)  # [32,14,90]\n",
    "\n",
    "        # è®¡ç®—æ¯ä¸ªå·ç§¯å±‚çš„è¾“å‡º\n",
    "        x1 = F.gelu(self.bn1(self.conv1(x)))  # å·ç§¯æ“ä½œ [32,512,90]\n",
    "        x2 = F.gelu(self.bn2(self.conv2(x)))  \n",
    "        x3 = F.gelu(self.bn3(self.conv3(x)))  \n",
    "\n",
    "        # åˆå¹¶ä¸‰ä¸ªå°ºåº¦çš„ç‰¹å¾å›¾\n",
    "        x_concat = torch.cat([x1, x2, x3], dim=1)  # åœ¨é€šé“ç»´åº¦æ‹¼æ¥ [32, 1536, 90]\n",
    "\n",
    "        # å¯¹æ‹¼æ¥åçš„ç‰¹å¾åº”ç”¨ Dropout\n",
    "        x_concat = self.dropout(x_concat)\n",
    "        x_concat = x_concat.transpose(1, 2) # [32, 90, 1536]\n",
    "\n",
    "        # residual = self.residual_proj(x_concat.reshape(x_concat.shape[0], -1))\n",
    "        output = x_concat + residual\n",
    "        return self.dropout(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd28f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttentionModule(nn.Module):\n",
    "    \"\"\"æ¨¡å—2: æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶æ¨¡å—\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_size, input_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # å¤šå¤´æ³¨æ„åŠ›\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=input_dim,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # ä½ç½®ç¼–ç \n",
    "        self.pos_encoding = self._generate_pos_encoding(input_size, input_dim)\n",
    "        \n",
    "        # å±‚å½’ä¸€åŒ–\n",
    "        self.layer_norm1 = nn.LayerNorm(input_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(input_dim)\n",
    "        \n",
    "        # å‰é¦ˆç½‘ç»œ\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size * 4, input_dim)\n",
    "        )\n",
    "        # ğŸ”§ æ·»åŠ ç»´åº¦æŠ•å½±å±‚ï¼š768 â†’ 256\n",
    "        self.output_projection = nn.Linear(input_dim, hidden_size)\n",
    "        \n",
    "    def _generate_pos_encoding(self, seq_len, d_model):\n",
    "        pos_encoding = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len).unsqueeze(1).float()\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                           -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        return nn.Parameter(pos_encoding.unsqueeze(0), requires_grad=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # æ·»åŠ ä½ç½®ç¼–ç \n",
    "        x = x + self.pos_encoding[:, :x.size(1), :]\n",
    "        \n",
    "        # å¤šå¤´è‡ªæ³¨æ„åŠ›\n",
    "        attn_output, _ = self.multihead_attn(x, x, x)\n",
    "        x = self.layer_norm1(x + attn_output)\n",
    "        \n",
    "        # å‰é¦ˆç½‘ç»œ\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.layer_norm2(x + ffn_output)\n",
    "\n",
    "        # ğŸ”§ é™ç»´æŠ•å½±ï¼š768 â†’ 256\n",
    "        x = self.output_projection(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ceacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGRUFusionModule(nn.Module):\n",
    "    \"\"\"æ¨¡å—3: é•¿çŸ­æœŸè®°å¿†èåˆæ¨¡å—\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # LSTMç”¨äºé•¿æœŸä¾èµ–\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, \n",
    "            num_layers=2, batch_first=True, \n",
    "            dropout=dropout, bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # GRUç”¨äºçŸ­æœŸæ¨¡å¼\n",
    "        self.gru = nn.GRU(\n",
    "            input_size, hidden_size,\n",
    "            num_layers=2, batch_first=True,\n",
    "            dropout=dropout, bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # èåˆæƒé‡å­¦ä¹ \n",
    "        self.fusion_gate = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 4, hidden_size * 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # è¾“å‡ºæŠ•å½±\n",
    "        self.output_proj = nn.Linear(hidden_size * 4, hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTMè¾“å‡º\n",
    "        lstm_out, _ = self.lstm(x) # [32, 90, 256]\n",
    "        \n",
    "        # GRUè¾“å‡º\n",
    "        gru_out, _ = self.gru(x) # [32, 90, 256]\n",
    "        \n",
    "        # ç‰¹å¾èåˆ \n",
    "        combined = torch.cat([lstm_out, gru_out], dim=-1) # [32, 90, 512]\n",
    "        \n",
    "        # å­¦ä¹ èåˆæƒé‡\n",
    "        fusion_weights = self.fusion_gate(combined) # [32, 90, 512]\n",
    "        \n",
    "        # åŠ æƒèåˆ\n",
    "        fused = combined * fusion_weights # [32, 90, 512]\n",
    "        \n",
    "        # è¾“å‡ºæŠ•å½±\n",
    "        output = self.output_proj(fused) # [32, 90, 128]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "834c7d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureInteractionModule(nn.Module):\n",
    "    \"\"\"æ¨¡å—4: ç‰¹å¾äº¤äº’æ¨¡å—\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # è‡ªäº¤äº’å±‚\n",
    "        self.self_interaction = nn.MultiheadAttention(\n",
    "            embed_dim=input_size,\n",
    "            num_heads=4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # è·¨æ—¶é—´æ­¥äº¤äº’\n",
    "        self.temporal_conv = nn.Conv1d(\n",
    "            input_size, hidden_size,\n",
    "            kernel_size=5, padding=2\n",
    "        )\n",
    "        \n",
    "        # ç‰¹å¾é‡è¦æ€§è¯„ä¼°\n",
    "        self.importance_net = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_size, hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 4, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # è‡ªäº¤äº’\n",
    "        attn_out, _ = self.self_interaction(x, x, x)\n",
    "        \n",
    "        # è·¨æ—¶é—´æ­¥äº¤äº’\n",
    "        conv_out = self.temporal_conv(attn_out.transpose(1, 2))\n",
    "        conv_out = conv_out.transpose(1, 2)\n",
    "        \n",
    "        # ç‰¹å¾é‡è¦æ€§åŠ æƒ\n",
    "        importance_weights = self.importance_net(conv_out.transpose(1, 2))\n",
    "        importance_weights = importance_weights.unsqueeze(1)\n",
    "        \n",
    "        weighted_features = conv_out * importance_weights\n",
    "        \n",
    "        # æ®‹å·®è¿æ¥å’Œå½’ä¸€åŒ–\n",
    "        output = self.layer_norm(weighted_features)\n",
    "        \n",
    "        return self.dropout(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85666ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedPredictionHead(nn.Module):\n",
    "    def __init__(self, input_size, pred_len, output_features, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.output_features = output_features\n",
    "        \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size // 2, pred_len * output_features)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # å…¨å±€å¹³å‡æ± åŒ–\n",
    "        global_feature = torch.mean(x, dim=1)  # [batch_size, input_size]\n",
    "        \n",
    "        # é¢„æµ‹\n",
    "        predictions = self.predictor(global_feature)\n",
    "        predictions = predictions.view(-1, self.pred_len, self.output_features)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# ä½¿ç”¨æ–¹å¼\n",
    "# 90å¤©æ¨¡å‹\n",
    "short_prediction_head = FixedPredictionHead(\n",
    "    input_size=hidden_size, pred_len=90, output_features=14\n",
    ")\n",
    "\n",
    "# 365å¤©æ¨¡å‹  \n",
    "long_prediction_head = FixedPredictionHead(\n",
    "    input_size=hidden_size, pred_len=365, output_features=14\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cf2907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqPredictionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    æ”¹è¿›çš„åºåˆ—åˆ°åºåˆ—é¢„æµ‹å¤´\n",
    "    - æ”¯æŒTeacher Forcingè®­ç»ƒ\n",
    "    - ä½¿ç”¨åŒå‘ç¼–ç å™¨\n",
    "    - æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶\n",
    "    - è§£å†³ç»´åº¦åŒ¹é…é—®é¢˜\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, pred_len, output_features, hidden_size=None):\n",
    "        super().__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.output_features = output_features\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size or input_size\n",
    "        \n",
    "        # ç¼–ç å™¨ï¼šåŒå‘LSTMè·å–æ›´å¥½çš„ä¸Šä¸‹æ–‡è¡¨ç¤º\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.1,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # è§£ç å™¨ï¼šå•å‘LSTMç”Ÿæˆæœªæ¥åºåˆ—\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=output_features,  # è§£ç å™¨è¾“å…¥æ˜¯ç›®æ ‡ç‰¹å¾ç»´åº¦\n",
    "            hidden_size=self.hidden_size * 2,  # åŒ¹é…åŒå‘ç¼–ç å™¨çš„è¾“å‡º\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # çŠ¶æ€è½¬æ¢å±‚ï¼šå°†åŒå‘ç¼–ç å™¨çŠ¶æ€è½¬ä¸ºå•å‘è§£ç å™¨çŠ¶æ€\n",
    "        self.hidden_transform = nn.Linear(self.hidden_size * 2, self.hidden_size * 2)\n",
    "        self.cell_transform = nn.Linear(self.hidden_size * 2, self.hidden_size * 2)\n",
    "        \n",
    "        # è¾“å‡ºæŠ•å½±å±‚\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size * 2, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.hidden_size, output_features)\n",
    "        )\n",
    "        \n",
    "        # è¾“å…¥ç‰¹å¾è½¬æ¢å±‚ï¼ˆç”¨äºå¤„ç†ç»´åº¦ä¸åŒ¹é…ï¼‰\n",
    "        if input_size != output_features:\n",
    "            self.feature_transform = nn.Linear(input_size, output_features)\n",
    "        else:\n",
    "            self.feature_transform = nn.Identity()\n",
    "            \n",
    "        # åˆå§‹åŒ–æƒé‡\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"LSTMæƒé‡åˆå§‹åŒ–\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "                # LSTM forget gate bias è®¾ä¸º1\n",
    "                if 'bias_ih' in name:\n",
    "                    n = param.size(0)\n",
    "                    param.data[n//4:n//2].fill_(1.)\n",
    "    \n",
    "    def forward(self, x, target=None):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        Args:\n",
    "            x: ç¼–ç å™¨è¾“å…¥ [batch_size, seq_len, input_size]\n",
    "            target: è§£ç å™¨ç›®æ ‡ [batch_size, pred_len, output_features]ï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨ï¼‰\n",
    "        Returns:\n",
    "            predictions: [batch_size, pred_len, output_features]\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 1. ç¼–ç é˜¶æ®µ\n",
    "        encoder_outputs, (encoder_hidden, encoder_cell) = self.encoder(x)\n",
    "        \n",
    "        # 2. çŠ¶æ€è½¬æ¢ï¼šåŒå‘ -> å•å‘\n",
    "        # encoder_hidden: [4, batch, hidden_size] -> [2, batch, hidden_size*2]\n",
    "        encoder_hidden = encoder_hidden.view(2, 2, batch_size, self.hidden_size)\n",
    "        encoder_hidden = torch.cat([encoder_hidden[:, 0, :, :], encoder_hidden[:, 1, :, :]], dim=2)\n",
    "        \n",
    "        encoder_cell = encoder_cell.view(2, 2, batch_size, self.hidden_size)  \n",
    "        encoder_cell = torch.cat([encoder_cell[:, 0, :, :], encoder_cell[:, 1, :, :]], dim=2)\n",
    "        \n",
    "        # åº”ç”¨çŠ¶æ€è½¬æ¢\n",
    "        decoder_hidden = self.hidden_transform(encoder_hidden)\n",
    "        decoder_cell = self.cell_transform(encoder_cell)\n",
    "        \n",
    "        # 3. è§£ç é˜¶æ®µ\n",
    "        predictions = []\n",
    "        \n",
    "        # åˆå§‹è§£ç å™¨è¾“å…¥ï¼šä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥å¹¶è½¬æ¢ç»´åº¦\n",
    "        last_input = x[:, -1:, :]  # [batch_size, 1, input_size]\n",
    "        decoder_input = self.feature_transform(last_input)  # [batch_size, 1, output_features]\n",
    "        \n",
    "        for t in range(self.pred_len):\n",
    "            # LSTMè§£ç \n",
    "            decoder_output, (decoder_hidden, decoder_cell) = self.decoder(\n",
    "                decoder_input, (decoder_hidden, decoder_cell)\n",
    "            )\n",
    "            \n",
    "            # ç”Ÿæˆé¢„æµ‹\n",
    "            pred = self.output_proj(decoder_output)  # [batch_size, 1, output_features]\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            # å‡†å¤‡ä¸‹ä¸€æ—¶é—´æ­¥è¾“å…¥\n",
    "            if self.training and target is not None and t < self.pred_len - 1:\n",
    "                # è®­ç»ƒæ—¶ï¼šéšæœºä½¿ç”¨Teacher Forcing\n",
    "                use_teacher_forcing = torch.rand(1).item() < 0.7\n",
    "                if use_teacher_forcing:\n",
    "                    decoder_input = target[:, t:t+1, :]\n",
    "                else:\n",
    "                    decoder_input = pred.detach()  # ä½¿ç”¨é¢„æµ‹å€¼ï¼Œé˜»æ–­æ¢¯åº¦\n",
    "            else:\n",
    "                # æ¨ç†æ—¶ï¼šä½¿ç”¨æ¨¡å‹é¢„æµ‹\n",
    "                decoder_input = pred\n",
    "        \n",
    "        # åˆå¹¶æ‰€æœ‰é¢„æµ‹\n",
    "        output = torch.cat(predictions, dim=1)  # [batch_size, pred_len, output_features]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41a54147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 90, 14])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "input_feature = 14\n",
    "input_size = 90\n",
    "dropout = 0.1\n",
    "output_size = 90\n",
    "model = MultiScaleCNN_TimeSeries(input_feature=14, hidden_size=128)\n",
    "temporal_attention = TemporalAttentionModule(hidden_size * 3, hidden_size, input_size)\n",
    "memory_fusion = LSTMGRUFusionModule(hidden_size, hidden_size, dropout)\n",
    "feature_interaction = FeatureInteractionModule(hidden_size, hidden_size, dropout)\n",
    "seq2seq_predictor = Seq2SeqPredictionHead(\n",
    "            input_size=hidden_size,\n",
    "            pred_len=output_size,\n",
    "            output_features=14,\n",
    "            hidden_size=hidden_size//2\n",
    "        )\n",
    "\n",
    "for input, label in dataloader:\n",
    "    output = model(input) # [32, 90, 384] 128*3=384\n",
    "    output = temporal_attention(output) # [32, 90, 128]\n",
    "    output = memory_fusion(output) # [32, 90, 128]\n",
    "    output = seq2seq_predictor(output) # [32, 90, 14]\n",
    "    print(output.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
