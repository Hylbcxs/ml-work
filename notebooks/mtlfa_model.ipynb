{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b27c6e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c26ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTLFA_PowerDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, input_size, prediction_type='short', scaler_path=None):\n",
    "        self.input_size = input_size\n",
    "        self.prediction_type = prediction_type\n",
    "        self.output_size = 90 if prediction_type == 'short' else 365\n",
    "        self.scaler_path = scaler_path or f\"/opt/data/private/hyl/code/ml-work/data/scaler.pkl\"\n",
    "        \n",
    "        # 加载和预处理数据\n",
    "        self._load_and_preprocess_data()\n",
    "        \n",
    "        # 生成样本\n",
    "        self.data_x = []\n",
    "        self.data_y = []\n",
    "        self._prepare_data()\n",
    "\n",
    "    def _load_and_preprocess_data(self):\n",
    "        \"\"\"加载和预处理数据，确保训练和测试使用相同的标准化参数\"\"\"\n",
    "            # 训练模式：加载训练数据并拟合scaler\n",
    "        train_df = pd.read_csv(\"/opt/data/private/hyl/code/ml-work/data/train_new.csv\")\n",
    "        train_df = self._clean_data(train_df)\n",
    "        \n",
    "        # 拟合scaler并保存\n",
    "        self.scaler = RobustScaler()  # 使用RobustScaler，对异常值更鲁棒\n",
    "        train_data = self.scaler.fit_transform(np.array(train_df))\n",
    "        \n",
    "        # 保存scaler参数\n",
    "        os.makedirs(os.path.dirname(self.scaler_path), exist_ok=True)\n",
    "        with open(self.scaler_path, 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        \n",
    "        self.data = train_data\n",
    "        print(f\"✅ 训练模式：拟合并保存scaler到 {self.scaler_path}\")\n",
    "        \n",
    "        # 数据统计\n",
    "        print(f\"📊 数据形状: {self.data.shape}\")\n",
    "        print(f\"📊 数据范围: [{self.data.min():.3f}, {self.data.max():.3f}]\")\n",
    "        print(f\"📊 数据均值: {self.data.mean():.3f}\")\n",
    "        print(f\"📊 数据标准差: {self.data.std():.3f}\")\n",
    "\n",
    "    def _clean_data(self, df):\n",
    "        \"\"\"数据清理\"\"\"\n",
    "        df.replace('?', np.nan, inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        df = df.drop(columns=['DateTime'])\n",
    "        return df\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        \"\"\"准备时间序列样本\"\"\"\n",
    "        window_size = self.input_size + self.output_size\n",
    "        \n",
    "        for index in range(len(self.data) - window_size + 1):\n",
    "            input_data = self.data[index:index + self.input_size]\n",
    "            label_data = self.data[index + self.input_size:index + window_size]\n",
    "            \n",
    "            self.data_x.append(input_data)\n",
    "            self.data_y.append(label_data)\n",
    "        \n",
    "        self.data_x = np.array(self.data_x)\n",
    "        self.data_y = np.array(self.data_y)\n",
    "        \n",
    "        print(f\"📊 {self.prediction_type}期预测数据准备完成\")\n",
    "        print(f\"📊 生成样本数: {len(self.data_x)}\")\n",
    "        print(f\"📊 输入形状: {self.data_x.shape}\")  # 应该是 [N, 90, 14]\n",
    "        print(f\"📊 标签形状: {self.data_y.shape}\")  # 应该是 [N, 90, 14]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_data = torch.tensor(self.data_x[idx], dtype=torch.float32)\n",
    "        label_data = torch.tensor(self.data_y[idx], dtype=torch.float32)\n",
    "        \n",
    "        return input_data, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7223982d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 训练模式：拟合并保存scaler到 /opt/data/private/hyl/code/ml-work/data/scaler.pkl\n",
      "📊 数据形状: (745, 14)\n",
      "📊 数据范围: [-5.668, 15.450]\n",
      "📊 数据均值: 0.125\n",
      "📊 数据标准差: 1.013\n",
      "📊 short期预测数据准备完成\n",
      "📊 生成样本数: 566\n",
      "📊 输入形状: (566, 90, 14)\n",
      "📊 标签形状: (566, 90, 14)\n",
      "torch.Size([32, 90, 14]) torch.Size([32, 90, 14])\n"
     ]
    }
   ],
   "source": [
    "input_size, output_size = 90, 90\n",
    "batch_size = 32\n",
    "raw_dataset = MTLFA_PowerDataset(input_size)\n",
    "dataloader = DataLoader(raw_dataset, batch_size=batch_size, drop_last=True)\n",
    "for input, label in dataloader:\n",
    "    print(input.shape, label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3edc04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleCNN_TimeSeries(nn.Module):\n",
    "    def __init__(self, input_feature, hidden_size, kernel_sizes=[3, 7, 15], dropout=0.3):\n",
    "        super(MultiScaleCNN_TimeSeries, self).__init__()\n",
    "        \n",
    "        # 使用多个卷积核来构建多尺度卷积模块\n",
    "        # 注意：我们将卷积沿着时间步(seq_len)进行应用\n",
    "        self.conv1 = nn.Conv1d(input_feature, hidden_size, kernel_size=kernel_sizes[0], padding=kernel_sizes[0] // 2)\n",
    "        self.conv2 = nn.Conv1d(input_feature, hidden_size, kernel_size=kernel_sizes[1], padding=kernel_sizes[1] // 2)\n",
    "        self.conv3 = nn.Conv1d(input_feature, hidden_size, kernel_size=kernel_sizes[2], padding=kernel_sizes[2] // 2)\n",
    "\n",
    "        # Batch normalization 和 Dropout 层\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.residual_proj = nn.Linear(input_feature, hidden_size * 3)\n",
    "        # 全连接层（如果需要的话）\n",
    "        # self.fc = nn.Linear(hidden_size * 3 * 90, 128)  # 假设序列长度是90，合并三个尺度的特征后\n",
    "        # self.output_fc = nn.Linear(128, 1)  # 输出层\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual_proj(x)\n",
    "        # x.shape = [batch_size, input_size, input_feature] --> [batch_size, input_feature, input_size] for Conv1D\n",
    "        x = x.transpose(1, 2)  # [32,14,90]\n",
    "\n",
    "        # 计算每个卷积层的输出\n",
    "        x1 = F.gelu(self.bn1(self.conv1(x)))  # 卷积操作 [32,512,90]\n",
    "        x2 = F.gelu(self.bn2(self.conv2(x)))  \n",
    "        x3 = F.gelu(self.bn3(self.conv3(x)))  \n",
    "\n",
    "        # 合并三个尺度的特征图\n",
    "        x_concat = torch.cat([x1, x2, x3], dim=1)  # 在通道维度拼接 [32, 1536, 90]\n",
    "\n",
    "        # 对拼接后的特征应用 Dropout\n",
    "        x_concat = self.dropout(x_concat)\n",
    "        x_concat = x_concat.transpose(1, 2) # [32, 90, 1536]\n",
    "\n",
    "        # residual = self.residual_proj(x_concat.reshape(x_concat.shape[0], -1))\n",
    "        output = x_concat + residual\n",
    "        return self.dropout(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd28f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttentionModule(nn.Module):\n",
    "    \"\"\"模块2: 时间注意力机制模块\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_size, input_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # 多头注意力\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=input_dim,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 位置编码\n",
    "        self.pos_encoding = self._generate_pos_encoding(input_size, input_dim)\n",
    "        \n",
    "        # 层归一化\n",
    "        self.layer_norm1 = nn.LayerNorm(input_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(input_dim)\n",
    "        \n",
    "        # 前馈网络\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size * 4, input_dim)\n",
    "        )\n",
    "        # 🔧 添加维度投影层：768 → 256\n",
    "        self.output_projection = nn.Linear(input_dim, hidden_size)\n",
    "        \n",
    "    def _generate_pos_encoding(self, seq_len, d_model):\n",
    "        pos_encoding = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len).unsqueeze(1).float()\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                           -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        return nn.Parameter(pos_encoding.unsqueeze(0), requires_grad=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 添加位置编码\n",
    "        x = x + self.pos_encoding[:, :x.size(1), :]\n",
    "        \n",
    "        # 多头自注意力\n",
    "        attn_output, _ = self.multihead_attn(x, x, x)\n",
    "        x = self.layer_norm1(x + attn_output)\n",
    "        \n",
    "        # 前馈网络\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.layer_norm2(x + ffn_output)\n",
    "\n",
    "        # 🔧 降维投影：768 → 256\n",
    "        x = self.output_projection(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ceacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGRUFusionModule(nn.Module):\n",
    "    \"\"\"模块3: 长短期记忆融合模块\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # LSTM用于长期依赖\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, \n",
    "            num_layers=2, batch_first=True, \n",
    "            dropout=dropout, bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # GRU用于短期模式\n",
    "        self.gru = nn.GRU(\n",
    "            input_size, hidden_size,\n",
    "            num_layers=2, batch_first=True,\n",
    "            dropout=dropout, bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # 融合权重学习\n",
    "        self.fusion_gate = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 4, hidden_size * 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # 输出投影\n",
    "        self.output_proj = nn.Linear(hidden_size * 4, hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM输出\n",
    "        lstm_out, _ = self.lstm(x) # [32, 90, 256]\n",
    "        \n",
    "        # GRU输出\n",
    "        gru_out, _ = self.gru(x) # [32, 90, 256]\n",
    "        \n",
    "        # 特征融合 \n",
    "        combined = torch.cat([lstm_out, gru_out], dim=-1) # [32, 90, 512]\n",
    "        \n",
    "        # 学习融合权重\n",
    "        fusion_weights = self.fusion_gate(combined) # [32, 90, 512]\n",
    "        \n",
    "        # 加权融合\n",
    "        fused = combined * fusion_weights # [32, 90, 512]\n",
    "        \n",
    "        # 输出投影\n",
    "        output = self.output_proj(fused) # [32, 90, 128]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "834c7d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureInteractionModule(nn.Module):\n",
    "    \"\"\"模块4: 特征交互模块\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 自交互层\n",
    "        self.self_interaction = nn.MultiheadAttention(\n",
    "            embed_dim=input_size,\n",
    "            num_heads=4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 跨时间步交互\n",
    "        self.temporal_conv = nn.Conv1d(\n",
    "            input_size, hidden_size,\n",
    "            kernel_size=5, padding=2\n",
    "        )\n",
    "        \n",
    "        # 特征重要性评估\n",
    "        self.importance_net = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_size, hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 4, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 自交互\n",
    "        attn_out, _ = self.self_interaction(x, x, x)\n",
    "        \n",
    "        # 跨时间步交互\n",
    "        conv_out = self.temporal_conv(attn_out.transpose(1, 2))\n",
    "        conv_out = conv_out.transpose(1, 2)\n",
    "        \n",
    "        # 特征重要性加权\n",
    "        importance_weights = self.importance_net(conv_out.transpose(1, 2))\n",
    "        importance_weights = importance_weights.unsqueeze(1)\n",
    "        \n",
    "        weighted_features = conv_out * importance_weights\n",
    "        \n",
    "        # 残差连接和归一化\n",
    "        output = self.layer_norm(weighted_features)\n",
    "        \n",
    "        return self.dropout(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85666ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedPredictionHead(nn.Module):\n",
    "    def __init__(self, input_size, pred_len, output_features, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.output_features = output_features\n",
    "        \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size // 2, pred_len * output_features)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 全局平均池化\n",
    "        global_feature = torch.mean(x, dim=1)  # [batch_size, input_size]\n",
    "        \n",
    "        # 预测\n",
    "        predictions = self.predictor(global_feature)\n",
    "        predictions = predictions.view(-1, self.pred_len, self.output_features)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# 使用方式\n",
    "# 90天模型\n",
    "short_prediction_head = FixedPredictionHead(\n",
    "    input_size=hidden_size, pred_len=90, output_features=14\n",
    ")\n",
    "\n",
    "# 365天模型  \n",
    "long_prediction_head = FixedPredictionHead(\n",
    "    input_size=hidden_size, pred_len=365, output_features=14\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cf2907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqPredictionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    改进的序列到序列预测头\n",
    "    - 支持Teacher Forcing训练\n",
    "    - 使用双向编码器\n",
    "    - 添加注意力机制\n",
    "    - 解决维度匹配问题\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, pred_len, output_features, hidden_size=None):\n",
    "        super().__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.output_features = output_features\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size or input_size\n",
    "        \n",
    "        # 编码器：双向LSTM获取更好的上下文表示\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.1,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # 解码器：单向LSTM生成未来序列\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=output_features,  # 解码器输入是目标特征维度\n",
    "            hidden_size=self.hidden_size * 2,  # 匹配双向编码器的输出\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # 状态转换层：将双向编码器状态转为单向解码器状态\n",
    "        self.hidden_transform = nn.Linear(self.hidden_size * 2, self.hidden_size * 2)\n",
    "        self.cell_transform = nn.Linear(self.hidden_size * 2, self.hidden_size * 2)\n",
    "        \n",
    "        # 输出投影层\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size * 2, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.hidden_size, output_features)\n",
    "        )\n",
    "        \n",
    "        # 输入特征转换层（用于处理维度不匹配）\n",
    "        if input_size != output_features:\n",
    "            self.feature_transform = nn.Linear(input_size, output_features)\n",
    "        else:\n",
    "            self.feature_transform = nn.Identity()\n",
    "            \n",
    "        # 初始化权重\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"LSTM权重初始化\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "                # LSTM forget gate bias 设为1\n",
    "                if 'bias_ih' in name:\n",
    "                    n = param.size(0)\n",
    "                    param.data[n//4:n//2].fill_(1.)\n",
    "    \n",
    "    def forward(self, x, target=None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        Args:\n",
    "            x: 编码器输入 [batch_size, seq_len, input_size]\n",
    "            target: 解码器目标 [batch_size, pred_len, output_features]（训练时使用）\n",
    "        Returns:\n",
    "            predictions: [batch_size, pred_len, output_features]\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 1. 编码阶段\n",
    "        encoder_outputs, (encoder_hidden, encoder_cell) = self.encoder(x)\n",
    "        \n",
    "        # 2. 状态转换：双向 -> 单向\n",
    "        # encoder_hidden: [4, batch, hidden_size] -> [2, batch, hidden_size*2]\n",
    "        encoder_hidden = encoder_hidden.view(2, 2, batch_size, self.hidden_size)\n",
    "        encoder_hidden = torch.cat([encoder_hidden[:, 0, :, :], encoder_hidden[:, 1, :, :]], dim=2)\n",
    "        \n",
    "        encoder_cell = encoder_cell.view(2, 2, batch_size, self.hidden_size)  \n",
    "        encoder_cell = torch.cat([encoder_cell[:, 0, :, :], encoder_cell[:, 1, :, :]], dim=2)\n",
    "        \n",
    "        # 应用状态转换\n",
    "        decoder_hidden = self.hidden_transform(encoder_hidden)\n",
    "        decoder_cell = self.cell_transform(encoder_cell)\n",
    "        \n",
    "        # 3. 解码阶段\n",
    "        predictions = []\n",
    "        \n",
    "        # 初始解码器输入：使用最后一个时间步并转换维度\n",
    "        last_input = x[:, -1:, :]  # [batch_size, 1, input_size]\n",
    "        decoder_input = self.feature_transform(last_input)  # [batch_size, 1, output_features]\n",
    "        \n",
    "        for t in range(self.pred_len):\n",
    "            # LSTM解码\n",
    "            decoder_output, (decoder_hidden, decoder_cell) = self.decoder(\n",
    "                decoder_input, (decoder_hidden, decoder_cell)\n",
    "            )\n",
    "            \n",
    "            # 生成预测\n",
    "            pred = self.output_proj(decoder_output)  # [batch_size, 1, output_features]\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            # 准备下一时间步输入\n",
    "            if self.training and target is not None and t < self.pred_len - 1:\n",
    "                # 训练时：随机使用Teacher Forcing\n",
    "                use_teacher_forcing = torch.rand(1).item() < 0.7\n",
    "                if use_teacher_forcing:\n",
    "                    decoder_input = target[:, t:t+1, :]\n",
    "                else:\n",
    "                    decoder_input = pred.detach()  # 使用预测值，阻断梯度\n",
    "            else:\n",
    "                # 推理时：使用模型预测\n",
    "                decoder_input = pred\n",
    "        \n",
    "        # 合并所有预测\n",
    "        output = torch.cat(predictions, dim=1)  # [batch_size, pred_len, output_features]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41a54147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 90, 14])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "input_feature = 14\n",
    "input_size = 90\n",
    "dropout = 0.1\n",
    "output_size = 90\n",
    "model = MultiScaleCNN_TimeSeries(input_feature=14, hidden_size=128)\n",
    "temporal_attention = TemporalAttentionModule(hidden_size * 3, hidden_size, input_size)\n",
    "memory_fusion = LSTMGRUFusionModule(hidden_size, hidden_size, dropout)\n",
    "feature_interaction = FeatureInteractionModule(hidden_size, hidden_size, dropout)\n",
    "seq2seq_predictor = Seq2SeqPredictionHead(\n",
    "            input_size=hidden_size,\n",
    "            pred_len=output_size,\n",
    "            output_features=14,\n",
    "            hidden_size=hidden_size//2\n",
    "        )\n",
    "\n",
    "for input, label in dataloader:\n",
    "    output = model(input) # [32, 90, 384] 128*3=384\n",
    "    output = temporal_attention(output) # [32, 90, 128]\n",
    "    output = memory_fusion(output) # [32, 90, 128]\n",
    "    output = seq2seq_predictor(output) # [32, 90, 14]\n",
    "    print(output.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
